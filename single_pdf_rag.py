from langgraph.graph import StateGraph
from langchain.chat_models import init_chat_model
from dotenv import load_dotenv
from typing import TypedDict, List, Any
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
from langchain_core.prompts import ChatPromptTemplate


load_dotenv()

embedding = DashScopeEmbeddings(
            model="text-embedding-v1", 
        )

class RagGraph(TypedDict):
    input: str
    pdf_path: str
    pdf_content: Any
    chunks: list
    vector_store: Any
    retrieved_docs: List[dict]
    output: str
    cache_exists: bool  # æ–°å¢ï¼šç¼“å­˜çŠ¶æ€æ ‡è®°


# æ£€æŸ¥ç¼“å­˜çŠ¶æ€
def check_cache(state: RagGraph):
    """æ£€æŸ¥å‘é‡å­˜å‚¨ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
    cache_exists = os.path.exists("vector_store") and os.path.exists("vector_store/index.faiss")
    state["cache_exists"] = cache_exists
    if cache_exists:
        print("âœ… å‘ç°å·²æœ‰å‘é‡å­˜å‚¨ç¼“å­˜")
    else:
        print("ğŸ”„ æœªå‘ç°ç¼“å­˜ï¼Œéœ€è¦å¤„ç†æ–‡æ¡£")
    return state

def pdf_read(state: RagGraph):
    print("æ­£åœ¨é˜…è¯»PDFæ–‡æ¡£...")
    try:
        text = ""
        reader = pdfplumber.open(state["pdf_path"])
        for page in reader.pages:
            text += page.extract_text()
        reader.close()
        state["pdf_content"] = text
    except Exception as e:
        print(f"è¯»å–PDFæ–‡æ¡£æ—¶å‡ºé”™: {e}")
        state["pdf_content"] = None
    return state
    

def get_chunks(state: RagGraph):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size = 1000,
        chunk_overlap = 200
    )
    chunks = text_splitter.split_text(state["pdf_content"]) 
    state["chunks"] = chunks
    print("æ–‡æ¡£å·²åˆ†å—")
    return state

def vector_store_func(state: RagGraph):
    vector_store = FAISS.from_texts(
        texts=state["chunks"],
        embedding=embedding
    )
    vector_store.save_local("vector_store")
    state["vector_store"] = vector_store
    print("å‘é‡ç´¢å¼•å·²æ„å»º")
    return state

def retrieve(state: RagGraph):
    """ç»Ÿä¸€çš„æ£€ç´¢å‡½æ•°"""
    new_db = FAISS.load_local("vector_store", 
    embeddings = embedding,
    allow_dangerous_deserialization=True
    ) 
    retriever = new_db.as_retriever()
    docs = retriever.invoke(state["input"])
    print(f"æ£€ç´¢åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
    state["retrieved_docs"] = docs
    return state

def ai_answer(state: RagGraph) -> RagGraph:
    """æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ç”ŸæˆAIå›ç­”"""
    print("æ­£åœ¨ç”Ÿæˆå›ç­”...")
    
    question = state.get("input", "")
    docs = state.get("retrieved_docs", [])
    
    context = "\n\n".join(
        doc.page_content if hasattr(doc, 'page_content') 
        else doc.get('content', str(doc)) 
        for doc in docs
    ) if docs else ""

    try:
        if context:
            llm = init_chat_model("deepseek-chat")
            prompt = f"""æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š
            
{context[:3000]}

é—®é¢˜ï¼š{question}
è¦æ±‚ï¼š
1. åŸºäºä¸Šä¸‹æ–‡å›ç­”
2. ä¿æŒä¸“ä¸šå‡†ç¡®
3. ç”¨ä¸­æ–‡å›ç­”"""
            
            response = llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)
        else:
            answer = f"æœªæ‰¾åˆ°ä¸ã€Œ{question}ã€ç›¸å…³çš„æ–‡æ¡£å†…å®¹"
            
    except Exception as e:
        print(f"ç”Ÿæˆå¤±è´¥: {e}")
        if context:
            keywords = [w for w in question.replace('?', '').replace('ï¼Ÿ', '').split() if w]
            found = [s for s in context.split('ã€‚') if any(k in s for k in keywords)][:3]
            answer = "æ‰¾åˆ°ç›¸å…³å†…å®¹ï¼š\n" + "\n".join(set(found)) if found else "æ–‡æ¡£ä¸­æœªæ‰¾åˆ°ç›´æ¥ç­”æ¡ˆ"
        else:
            answer = "æ— å¯ç”¨æ–‡æ¡£å†…å®¹"
    
    state["output"] = answer
    return state

# æ¡ä»¶è¾¹å‡½æ•°ï¼šå†³å®šæ˜¯å¦éœ€è¦æ–‡æ¡£é¢„å¤„ç†
def should_process_document(state: RagGraph) -> str:
    """æ¡ä»¶è¾¹ï¼šæ ¹æ®ç¼“å­˜çŠ¶æ€å†³å®šä¸‹ä¸€æ­¥"""
    if state["cache_exists"]:
        return "retrieve"  # æœ‰ç¼“å­˜ï¼Œç›´æ¥æ£€ç´¢
    else:
        return "pdf_read"  # æ— ç¼“å­˜ï¼Œéœ€è¦å¤„ç†æ–‡æ¡£

# åˆ›å»ºæ™ºèƒ½RAGå·¥ä½œæµ
def create_smart_rag():
    workflow = StateGraph(RagGraph)
    
    # æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
    workflow.add_node("check_cache", check_cache)
    workflow.add_node("pdf_read", pdf_read)
    workflow.add_node("get_chunks", get_chunks)
    workflow.add_node("vector_store", vector_store_func)
    workflow.add_node("retrieve", retrieve)
    workflow.add_node("ai_answer", ai_answer)
    
    # è®¾ç½®æ¡ä»¶è¾¹ï¼šä»ç¼“å­˜æ£€æŸ¥å¼€å§‹åˆ†æ”¯
    workflow.add_conditional_edges(
        "check_cache",
        should_process_document,
        {
            "retrieve": "retrieve",      # æœ‰ç¼“å­˜ç›´æ¥æ£€ç´¢
            "pdf_read": "pdf_read"       # æ— ç¼“å­˜å…ˆå¤„ç†æ–‡æ¡£
        }
    )
    
    # æ–‡æ¡£å¤„ç†æµç¨‹çš„è¾¹
    workflow.add_edge("pdf_read", "get_chunks")
    workflow.add_edge("get_chunks", "vector_store")
    workflow.add_edge("vector_store", "retrieve")
    
    # æœ€ç»ˆå›ç­”
    workflow.add_edge("retrieve", "ai_answer")
    
    # è®¾ç½®å…¥å£å’Œå‡ºå£
    workflow.set_entry_point("check_cache")
    workflow.set_finish_point("ai_answer")
    
    return workflow.compile()

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    pdf_path = "D:\\Browser Download\\AppCacth\\Princess3\\princess.pdf"
    
    # åˆ›å»ºæ™ºèƒ½RAGä»£ç†
    smart_rag = create_smart_rag()
    
    print("ğŸ¤– æ™ºèƒ½RAGç³»ç»Ÿå¯åŠ¨ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰:")
    
    while True:
        question = input("\nâ“ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
        if question.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
            print("ğŸ‘‹ å†è§ï¼")
            break
        
        if not question:
            continue
            
        # åˆ›å»ºçŠ¶æ€
        state: RagGraph = {
            "input": question,
            "pdf_path": pdf_path,
            "pdf_content": None,
            "chunks": None,
            "vector_store": None,
            "retrieved_docs": None,
            "output": None,
            "cache_exists": False
        }
        
        # è¿è¡Œæ™ºèƒ½å·¥ä½œæµ
        try:
            result = smart_rag.invoke(state)
            print(f"\nğŸ¤– å›ç­”: {result['output']}")
        except Exception as e:
            print(f"âŒ å‡ºé”™äº†: {e}")