"""
å¤šPDFæ–‡æ¡£çš„RAGç³»ç»Ÿ
å°†å¤špdfç»Ÿä¸€åŠ è½½åˆ°å‘é‡åº“
æ”¯æŒæ£€ç´¢æ’åº, æ¥æºæ„ŸçŸ¥
"""
# æ ‡å‡†åº“å¯¼å…¥
import os
import io
import re
import base64
import json

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
from dotenv import load_dotenv
from PIL import Image
from openai import OpenAI
import fitz  # PyMuPDF
import pdfplumber

# LangChainç›¸å…³å¯¼å…¥
from typing import Annotated, TypedDict, List, Any, Dict, Optional
from langchain.chat_models import init_chat_model
from langchain_core.runnables import RunnableConfig
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# LangGraphç›¸å…³å¯¼å…¥
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages

# æ£€æŸ¥ç‚¹å¯¼å…¥
try:
    from langgraph.checkpoint.sqlite import SqliteSaver  # type: ignore
except ImportError:
    SqliteSaver = None  # type: ignore


load_dotenv()

# ===========================================
# é…ç½®å‚æ•°
# ===========================================
CONFIG = {
    "embedding_model": "text-embedding-v1",
    "chat_model": "deepseek-chat",
    "vision_model": "qwen-vl-plus",
    "checkpoint_file": "rag_sessions.db",
    "vector_store_path": "vector_store",
    "chunk_size": 1500,
    "chunk_overlap": 200,
    "max_chunk_length": 2000,
    "truncate_length": 1900,
    "retrieval_k": 10,
    "mmr_lambda": 0.25,
    "context_max_length": 4000,
    "text_threshold": 100,  # åˆ¤æ–­æ˜¯å¦éœ€è¦OCRçš„æ–‡å­—æ•°é‡é˜ˆå€¼
    "preview_pages": 3  # åˆ†ç±»æ—¶æ£€æŸ¥çš„é¡µæ•°
}

# åˆå§‹åŒ–ç»„ä»¶
embedding = DashScopeEmbeddings(model=CONFIG["embedding_model"])
checkpoint_file = CONFIG["checkpoint_file"]

# åˆå§‹åŒ–æ£€æŸ¥ç‚¹å­˜å‚¨
if SqliteSaver:
    try:
        memory = SqliteSaver.from_conn_string(f"sqlite:///{checkpoint_file}")
    except (AttributeError, TypeError):
        memory = SqliteSaver(checkpoint_file)
else:
    memory = None  # type: ignore

# ===========================================
# å·¥å…·å‡½æ•°
# ===========================================
def clear_session_data():
    """æ¸…ç†æ‰€æœ‰ä¼šè¯æ•°æ®"""
    if os.path.exists(checkpoint_file):
        os.remove(checkpoint_file)
        print(f"âœ… å·²æ¸…ç†ä¼šè¯æ•°æ®æ–‡ä»¶: {checkpoint_file}")
    else:
        print(f"ğŸ“ ä¼šè¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_file}")

# ===========================================
# çŠ¶æ€å®šä¹‰
# ===========================================
class RagGraph(TypedDict):
    input: str
    pdf_path: str
    pdf_content: Optional[str]
    chunks: Optional[List[Document]]
    vector_store: Optional[Any]
    retrieved_docs: Optional[List[Document]]
    output: Optional[str]
    cache_exists: bool
    pdf_classification: Dict[str, str]
    messages: Annotated[List[dict], add_messages]
    user_id: str

# ===========================================
# PDFå¤„ç†è¾…åŠ©å‡½æ•°
# ===========================================
def extract_with_qwenvl(image: Image.Image) -> Dict[str, Any]:
    """ä½¿ç”¨QwenVLè¿›è¡ŒOCRå’Œè¡¨æ ¼æå–"""
    buffered = io.BytesIO()
    image.save(buffered, format="PNG")
    img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
    
    client = OpenAI(
        api_key=os.getenv("DASHSCOPE_API_KEY"),
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    )
    
    completion = client.chat.completions.create(
        model=CONFIG["vision_model"],
        messages=[{
            "role": "user",
            "content": [
                {"type": "text", "text": "è¯·åˆ†æè¿™å¼ COAå›¾ç‰‡ï¼Œæå–æ£€éªŒé¡¹ç›®ã€æ ‡å‡†ã€ç»“æœç­‰ä¿¡æ¯ã€‚ä»¥JSONæ ¼å¼è¿”å›ï¼š{\"text\": \"æ–‡æœ¬å†…å®¹\", \"tables\": [{\"headers\": [...], \"rows\": [...]}]}"},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_base64}"}}
            ]
        }]
    )
    
    response_content = completion.choices[0].message.content  # type: ignore
    if response_content:
        try:
            result = json.loads(response_content)
        except json.JSONDecodeError:
            result = {"text": "", "tables": []}
    else:
        result = {"text": "", "tables": []}
    return {"text": result.get("text", ""), "tables": result.get("tables", [])}

# ===========================================
# æ ¸å¿ƒå¤„ç†å‡½æ•°
# ===========================================
def pdf_classify(state: RagGraph) -> RagGraph:
    """åˆ†ç±»PDFæ–‡æ¡£: åˆ¤æ–­æ˜¯å¦éœ€è¦OCRè¯†åˆ«"""
    print("ğŸ” å¼€å§‹PDFæ–‡æ¡£åˆ†ç±»...")
    pdf_folder = state['pdf_path']
    classification = {}
    
    for pdf_name in os.listdir(pdf_folder):
        if not pdf_name.endswith('.pdf'):
            continue
            
        pdf_path = os.path.join(pdf_folder, pdf_name)
        print(f"ğŸ“„ æ­£åœ¨åˆ†ç±»: {pdf_name}")
        
        try:
            with pdfplumber.open(pdf_path) as reader:
                text_content = ""
                # åªæ£€æŸ¥å‰å‡ é¡µæ¥åˆ¤æ–­
                for page in reader.pages[:CONFIG["preview_pages"]]:
                    page_text = page.extract_text()
                    if page_text:
                        text_content += page_text
            
            # æ ¹æ®æ–‡æœ¬é‡åˆ¤æ–­å¤„ç†æ–¹å¼
            if len(text_content.strip()) > CONFIG["text_threshold"]:
                classification[pdf_name] = "text_extractable"
                print(f"  âœ… {pdf_name} - å¯ç›´æ¥æå–æ–‡å­—")
            else:
                classification[pdf_name] = "vision_needed"
                print(f"  ğŸ” {pdf_name} - éœ€è¦è§†è§‰è¯†åˆ«")
                    
        except Exception as e:
            print(f"  âŒ {pdf_name} - åˆ†ç±»å¤±è´¥: {e}")
            classification[pdf_name] = "vision_needed"
    
    state['pdf_classification'] = classification
    print(f"ğŸ“Š åˆ†ç±»å®Œæˆï¼Œå…± {len(classification)} ä¸ªæ–‡æ¡£")
    return state


def process_pdfs(state: RagGraph) -> RagGraph:
    """ç»Ÿä¸€å¤„ç†PDFæ–‡æ¡£, æ ¹æ®åˆ†ç±»ç»“æœé€‰æ‹©å¤„ç†æ–¹å¼"""
    all_text = ""
    pdf_folder = state['pdf_path']
    classification = state.get('pdf_classification', {})
    
    for pdf_name in os.listdir(pdf_folder):
        if not pdf_name.endswith('.pdf'):
            continue
            
        pdf_path = os.path.join(pdf_folder, pdf_name)
        process_method = classification.get(pdf_name, "vision_needed")
        
        if process_method == "text_extractable":
            # ä½¿ç”¨pdfplumberæå–æ–‡æœ¬
            print(f"ğŸ“– ä½¿ç”¨pdfplumberå¤„ç†: {pdf_name}")
            try:
                with pdfplumber.open(pdf_path) as reader:
                    for page in reader.pages:  # type: ignore
                        page_text = page.extract_text()  # type: ignore
                        if page_text:
                            all_text += f"[æ¥æº:{pdf_name}|é¡µç :{page.page_number}]\n{page_text}\n\n"  # type: ignore
                    all_text += f"[END:{pdf_name}]\n"
            except Exception as e:
                print(f"âŒ pdfplumberå¤„ç†å¤±è´¥: {e}")
                
        elif process_method == "vision_needed":
            # ä½¿ç”¨QwenVLè¿›è¡ŒOCR
            print(f"ğŸ” ä½¿ç”¨QwenVLå¤„ç†: {pdf_name}")
            try:
                with fitz.open(pdf_path) as doc:  # type: ignore
                    for page_num in range(len(doc)):
                        page = doc[page_num]
                        
                        # è½¬æ¢ä¸ºå›¾åƒå¹¶è¿›è¡ŒOCR
                        mat = fitz.Matrix(2.0, 2.0)  # type: ignore
                        pix = page.get_pixmap(matrix=mat)  # type: ignore
                        img_data = pix.tobytes("png")  # type: ignore
                        image = Image.open(io.BytesIO(img_data))
                        
                        result = extract_with_qwenvl(image)
                        if result["text"]:
                            all_text += f"[æ¥æº:{pdf_name}|é¡µç :{page_num + 1}]\n{result['text']}\n\n"
                        
                        # å¤„ç†è¡¨æ ¼æ•°æ®
                        if result["tables"]:
                            for table in result["tables"]:
                                table_text = "\n".join(["\t".join(row) for row in [table.get("headers", [])] + table.get("rows", [])])
                                all_text += f"[æ¥æº:{pdf_name}|é¡µç :{page_num + 1}|è¡¨æ ¼]\n{table_text}\n\n"
                    all_text += f"[END:{pdf_name}]\n"
            except Exception as e:
                print(f"âŒ QwenVLå¤„ç†å¤±è´¥: {e}")
    
    state['pdf_content'] = all_text
    print(f"âœ… PDFå¤„ç†å®Œæˆï¼Œæ€»é•¿åº¦: {len(all_text)} å­—ç¬¦")
    return state

def check_cache(state: RagGraph) -> RagGraph:
    """æ£€æŸ¥å‘é‡å­˜å‚¨ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
    cache_path = CONFIG["vector_store_path"]
    cache_exists = os.path.exists(cache_path) and os.path.exists(f"{cache_path}/index.faiss")
    state["cache_exists"] = cache_exists
    print("âœ… å‘ç°å·²æœ‰å‘é‡å­˜å‚¨ç¼“å­˜" if cache_exists else "ğŸ”„ æœªå‘ç°ç¼“å­˜ï¼Œéœ€è¦å¤„ç†æ–‡æ¡£")
    return state

def get_chunks(state: RagGraph) -> RagGraph:
    """ä½¿ç”¨RecursiveCharacterTextSplitteråˆ†å—, ä¿ç•™å…ƒæ•°æ®"""
    raw_text = state["pdf_content"]
    if not raw_text:
        state["chunks"] = []
        print("ğŸ“ æ²¡æœ‰æ–‡æ¡£å†…å®¹éœ€è¦åˆ†å—")
        return state
    
    # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CONFIG["chunk_size"],
        chunk_overlap=CONFIG["chunk_overlap"],
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ";", ":", "ï¼Œ", " ", ""]
    )
    
    # æŒ‰æ–‡æ¡£åˆ†å‰²
    doc_sections = raw_text.split("[END:")
    documents = []
    
    for section in doc_sections:
        if not section.strip():
            continue
            
        # æå–æ¥æºä¿¡æ¯
        source_matches = re.findall(r'\[æ¥æº:([^|]+)\|é¡µç :(\d+)\]', section)
        
        if source_matches:
            pdf_name = source_matches[0][0]
            chunks = text_splitter.split_text(section)
            
            for chunk in chunks:
                # ä¸ºæ¯ä¸ªchunkæå–é¡µç ä¿¡æ¯
                page_matches = re.findall(r'\[æ¥æº:[^|]+\|é¡µç :(\d+)\]', chunk)
                page_num = page_matches[0] if page_matches else "æœªçŸ¥"
                
                # åˆ›å»ºDocumentå¯¹è±¡ï¼ŒåŒ…å«å…ƒæ•°æ®
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "source": pdf_name,
                        "page": page_num,
                        "chunk_id": len(documents)
                    }
                )
                documents.append(doc)
    
    state["chunks"] = documents
    print(f"ğŸ“ æ–‡æ¡£å·²åˆ†å—ï¼Œå…± {len(documents)} ä¸ªå—")
    return state

def vector_store_func(state: RagGraph) -> RagGraph:
    """æ„å»ºå‘é‡å­˜å‚¨, å¤„ç†Documentå¯¹è±¡"""
    documents = state.get("chunks") or []
    
    # å¤„ç†è¿‡é•¿æ–‡æ¡£
    valid_docs = []
    for doc in documents:
        if len(doc.page_content) <= CONFIG["max_chunk_length"]:
            valid_docs.append(doc)
        else:
            valid_docs.append(Document(
                page_content=doc.page_content[:CONFIG["truncate_length"]] + "...[æ–‡æœ¬è¢«æˆªæ–­]",
                metadata=doc.metadata
            ))
    
    if not valid_docs:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£å—")
        return state
    
    # æ„å»ºå‘é‡å­˜å‚¨
    vector_store = FAISS.from_documents(
        documents=valid_docs,
        embedding=embedding
    )
    vector_store.save_local(CONFIG["vector_store_path"])
    state["vector_store"] = None  # æ ‡è®°å‘é‡å­˜å‚¨å·²åˆ›å»º
    print(f"âœ… å‘é‡ç´¢å¼•å·²æ„å»ºï¼Œå¤„ç†äº† {len(valid_docs)} ä¸ªæ–‡æ¡£å—")
    
    return state

def retrieve(state: RagGraph) -> RagGraph:
    """ä»å‘é‡å­˜å‚¨ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
    print("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
    
    question = state.get("input", "")
    
    try:
        # æ¯æ¬¡éƒ½ä»ç£ç›˜åŠ è½½å‘é‡å­˜å‚¨ï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
        vector_store = FAISS.load_local(
            CONFIG["vector_store_path"], 
            embedding, 
            allow_dangerous_deserialization=True
        )
        print("âœ… æˆåŠŸåŠ è½½å·²æœ‰å‘é‡å­˜å‚¨")
        
        # æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢
        docs = vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={'k': CONFIG["retrieval_k"], 'lambda_mult': CONFIG["mmr_lambda"]}
        ).invoke(question)
        
        state["retrieved_docs"] = docs
        print(f"âœ… æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£æ¥æº
        sources = set()
        for doc in docs:
            if hasattr(doc, 'metadata') and 'source' in doc.metadata:
                sources.add(doc.metadata['source'])
        
        if sources:
            print(f"ğŸ“š æ–‡æ¡£æ¥æº: {', '.join(sources)}")
            
    except Exception as e:
        print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
        state["retrieved_docs"] = []
    
    return state

def ai_answer(state: RagGraph) -> RagGraph:
    """ç”ŸæˆAIç­”æ¡ˆ, åŸºäºæ£€ç´¢æ–‡æ¡£å’Œå¯¹è¯å†å²"""
    question = state.get("input", "")
    docs = state.get("retrieved_docs", []) or []
    messages = state.get("messages", [])
    
    # Build conversation history
    conversation_history = ""
    if len(messages) > 1:  # If there is historical dialogue
        for msg in messages[:-1]:  # Exclude current question
            # Handle both dict format and LangChain message objects
            if isinstance(msg, dict):
                # Dict format - ä¼˜å…ˆæ£€æŸ¥å­—å…¸ç±»å‹
                role = msg.get("role", "unknown")
                content = msg.get("content", "")
            elif hasattr(msg, 'content') and not isinstance(msg, dict):
                # LangChain message object - ç¡®ä¿ä¸æ˜¯å­—å…¸ç±»å‹
                role = msg.__class__.__name__.replace('Message', '').lower()
                content = msg.content
            else:
                # Fallback
                role = "unknown"
                content = str(msg)
            conversation_history += f"{role}: {content}\n"
    
    # æ„å»ºåŒ…å«å†å²çš„æç¤ºè¯
    # å¤„ç†æ–‡æ¡£å†…å®¹æå–ï¼Œå…¼å®¹ä¸åŒçš„æ–‡æ¡£ç±»å‹
    context_parts = []
    # ç¡®ä¿docsä¸ä¸ºNone
    if docs:
        for doc in docs:
            if hasattr(doc, 'page_content'):
                context_parts.append(doc.page_content)
            elif isinstance(doc, dict) and 'page_content' in doc:
                context_parts.append(doc['page_content'])
            elif isinstance(doc, str):
                context_parts.append(doc)
    
    context = "\n\n".join(context_parts)
    
    prompt = f"""å¯¹è¯å†å²: {conversation_history}ç›¸å…³æ–‡æ¡£: {context[:CONFIG["context_max_length"]]}å½“å‰é—®é¢˜: {question}è¦æ±‚:
1. åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹å’Œå¯¹è¯å†å²å›ç­”
2. ç”¨ä¸­æ–‡å›ç­”
3. ç®€æ´æ˜äº†"""
    
    llm = init_chat_model(CONFIG["chat_model"])
    response = llm.invoke(prompt)
    
    # ç¡®ä¿answerå§‹ç»ˆä¸ºå­—ç¬¦ä¸²ç±»å‹
    if hasattr(response, 'content'):
        content = response.content
        if isinstance(content, list):
            # å¦‚æœcontentæ˜¯åˆ—è¡¨ï¼Œå°†å…¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            answer = ' '.join(str(item) for item in content)
        else:
            answer = str(content) if content is not None else ""
    else:
        answer = str(response)
    
    # Add AI answer to message history - let LangGraph handle message objects automatically
    state["output"] = answer
    
    return state
# ===========================================
# è·¯ç”±å‡½æ•°
# ===========================================
def should_process_document(state: RagGraph) -> str:
    """æ ¹æ®ç¼“å­˜çŠ¶æ€å†³å®šä¸‹ä¸€æ­¥"""
    return "retrieve" if state["cache_exists"] else "pdf_classify"

# ===========================================
# å·¥ä½œæµæ„å»º
# ===========================================
def create_smart_rag():
    """åˆ›å»ºæ™ºèƒ½RAGå·¥ä½œæµ"""
    workflow = StateGraph(RagGraph)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("check_cache", check_cache)
    workflow.add_node("pdf_classify", pdf_classify)
    workflow.add_node("process_pdfs", process_pdfs)
    workflow.add_node("get_chunks", get_chunks)
    workflow.add_node("vector_store", vector_store_func)
    workflow.add_node("retrieve", retrieve)
    workflow.add_node("ai_answer", ai_answer)
    
    # æ·»åŠ è¾¹å’Œæ¡ä»¶è¾¹
    workflow.add_conditional_edges(
        "check_cache",
        should_process_document,
        {"retrieve": "retrieve", "pdf_classify": "pdf_classify"}
    )
    
    # ç®€åŒ–çš„çº¿æ€§æµç¨‹
    workflow.add_edge("pdf_classify", "process_pdfs")
    workflow.add_edge("process_pdfs", "get_chunks")
    workflow.add_edge("get_chunks", "vector_store")
    workflow.add_edge("vector_store", "retrieve")
    workflow.add_edge("retrieve", "ai_answer")
    
    workflow.set_entry_point("check_cache")
    workflow.set_finish_point("ai_answer")
    
    return workflow.compile(checkpointer=memory)

# ===========================================
# ä¸»ç¨‹åº
# ===========================================
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    pdf_path = r"C:\Users\Yu\Desktop\coa\COA"
    user_id = "user_123"

    # æ£€æŸ¥æ˜¯å¦æ¸…ç†ä¼šè¯æ•°æ®
    clear_sessions = input("ğŸ—„ï¸ æ˜¯å¦æ¸…ç†æ—§çš„ä¼šè¯æ•°æ®? (y/N): ").strip().lower()
    if clear_sessions in ['y', 'yes', 'æ˜¯']:
        clear_session_data()

    # åˆå§‹åŒ–ç³»ç»Ÿ
    smart_rag = create_smart_rag()
    print("ğŸ¤– æ™ºèƒ½RAGç³»ç»Ÿå¯åŠ¨ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
    print("ğŸ’¡ çº¿ç¨‹åŠŸèƒ½å·²å¯ç”¨ï¼Œç³»ç»Ÿä¼šè®°ä½å¯¹è¯å†å²")
    
    config = RunnableConfig(configurable={"thread_id": user_id})

    # ä¸»å¾ªç¯
    while True:
        question = input("\nâ“ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
        if question.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
            break

        if not question:
            continue

        # æ„å»ºçŠ¶æ€
        state: RagGraph = {
            "input": question,
            "messages": [{"role": "user", "content": question}],
            "user_id": user_id,
            "pdf_path": pdf_path,
            "pdf_content": None,
            "chunks": None,
            "vector_store": None,
            "retrieved_docs": None,
            "output": None,
            "cache_exists": False,
            "pdf_classification": {},
        }

        try:
            result = smart_rag.invoke(state, config=config)
            print(f"\nğŸ¤– å›ç­”: {result['output']}")
        except Exception as e:
            print(f"âŒ å‡ºé”™äº†: {e}")
