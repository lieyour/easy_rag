"""
å¤šPDFæ–‡æ¡£çš„RAGç³»ç»Ÿ
å°†å¤špdfç»Ÿä¸€åŠ è½½åˆ°å‘é‡åº“
æ”¯æŒæ£€ç´¢æ’åºï¼Œæ¥æºæ„ŸçŸ¥
"""

from langgraph.graph import StateGraph
from langchain.chat_models import init_chat_model
from dotenv import load_dotenv
from typing import TypedDict, List, Any, Dict
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import fitz  # PyMuPDF
from PIL import Image
import io
import base64
import json
from openai import OpenAI
from langchain.schema import Document
import re
import dashscope
from http import HTTPStatus
from datetime import datetime


load_dotenv()

embedding = DashScopeEmbeddings(model="text-embedding-v2")


class RagGraph(TypedDict):
    input: str
    pdf_path: str
    pdf_content: Any
    chunks: list
    vector_store: Any
    retrieved_docs: List[dict]
    output: str
    cache_exists: bool
    pdf_classification: Dict[str, str]  # PDFåˆ†ç±»ç»“æœ
    json_cache_exists: bool  # JSONç¼“å­˜æ˜¯å¦å­˜åœ¨
    # è¯„ä¼°æ¨¡å¼ç›¸å…³å­—æ®µ
    evaluation_mode: bool
    test_questions: List[Dict]
    current_question_index: int
    evaluation_results: List[Dict]
    metrics: Dict[str, float]


def convert_page_to_image(page: fitz.Page) -> Image.Image:
    """å°†PDFé¡µé¢è½¬æ¢ä¸ºPILå›¾åƒ"""
    mat = fitz.Matrix(2.0, 2.0)
    pix = page.get_pixmap(matrix=mat)
    img_data = pix.tobytes("png")
    return Image.open(io.BytesIO(img_data))


def extract_with_qwenvl(image: Image.Image) -> Dict[str, Any]:
    """ä½¿ç”¨QwenVLè¿›è¡ŒOCRå’Œè¡¨æ ¼æå–"""
    buffered = io.BytesIO()
    image.save(buffered, format="PNG")
    img_base64 = base64.b64encode(buffered.getvalue()).decode("utf-8")

    client = OpenAI(
        api_key=os.getenv("DASHSCOPE_API_KEY"),
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    )

    completion = client.chat.completions.create(
        model="qwen-vl-plus-latest",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": 'è¯·åˆ†æè¿™å¼ COAå›¾ç‰‡ï¼Œæå–æ£€éªŒé¡¹ç›®ã€æ ‡å‡†ã€ç»“æœç­‰ä¿¡æ¯ã€‚ä»¥JSONæ ¼å¼è¿”å›ï¼š{"text": "æ–‡æœ¬å†…å®¹", "tables": [{"headers": [...], "rows": [...]}]}',
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/png;base64,{img_base64}"},
                    },
                ],
            }
        ],
    )

    response_content = completion.choices[0].message.content

    result = json.loads(response_content)
    return {"text": result.get("text", ""), "tables": result.get("tables", [])}


def pdf_classify(state: RagGraph):
    """åˆ†ç±»è·¯ç”±èŠ‚ç‚¹ï¼šéå†æ–‡ä»¶å¤¹ï¼Œå¯¹æ¯ä¸ªPDFè¿›è¡Œåˆ†ç±»"""
    print("ğŸ” å¼€å§‹PDFæ–‡æ¡£åˆ†ç±»...")
    pdf_folder = state["pdf_path"]
    classification = {}

    for pdf_name in os.listdir(pdf_folder):
        if not pdf_name.endswith(".pdf"):
            continue

        pdf_path = os.path.join(pdf_folder, pdf_name)

        # å°è¯•ä½¿ç”¨pdfplumberæå–æ–‡å­—
        with pdfplumber.open(pdf_path) as reader:
            text_content = ""
            for page in reader.pages[:3]:  # åªæ£€æŸ¥å‰3é¡µ
                page_text = page.extract_text()
                if page_text:
                    text_content += page_text

            # åˆ¤æ–­æ˜¯å¦èƒ½æå–åˆ°è¶³å¤Ÿçš„æ–‡å­—å†…å®¹
        if len(text_content.strip()) > 100:  # å¦‚æœæå–åˆ°è¶…è¿‡100ä¸ªå­—ç¬¦
            classification[pdf_name] = "text_extractable"

        else:
            classification[pdf_name] = "vision_needed"
    state["pdf_classification"] = classification
    print(f"ğŸ“Š åˆ†ç±»å®Œæˆï¼Œå…± {len(classification)} ä¸ªæ–‡æ¡£")
    return state


def check_cache(state: RagGraph):
    """æ£€æŸ¥å‘é‡å­˜å‚¨ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
    cache_exists = os.path.exists("vector_store") and os.path.exists(
        "vector_store/index.faiss"
    )
    state["cache_exists"] = cache_exists
    print("âœ… å‘ç°å·²æœ‰å‘é‡å­˜å‚¨ç¼“å­˜" if cache_exists else "ğŸ”„ æœªå‘ç°ç¼“å­˜ï¼Œéœ€è¦å¤„ç†æ–‡æ¡£")
    return state


def check_json_cache(state: RagGraph):
    """æ£€æŸ¥PDFå†…å®¹JSONç¼“å­˜æ˜¯å¦å­˜åœ¨"""
    json_file_path = "pdf_content.json"
    json_exists = os.path.exists(json_file_path)

    if json_exists:
        try:
            with open(json_file_path, "r", encoding="utf-8") as f:
                pdf_data = json.load(f)
                state["pdf_content"] = pdf_data.get("pdf_content", "")
                state["pdf_classification"] = pdf_data.get("pdf_classification", {})
            print(f"âœ… ä»JSONæ–‡ä»¶åŠ è½½PDFå†…å®¹ï¼Œé•¿åº¦: {len(state['pdf_content'])} å­—ç¬¦")
            state["json_cache_exists"] = True
        except Exception as e:
            print(f"âŒ è¯»å–JSONæ–‡ä»¶å¤±è´¥: {e}")
            state["json_cache_exists"] = False
    else:
        print("ğŸ”„ æœªå‘ç°JSONç¼“å­˜ï¼Œéœ€è¦é‡æ–°å¤„ç†PDF")
        state["json_cache_exists"] = False

    return state


def pdfread_qwen(state: RagGraph):
    """ä½¿ç”¨QwenVLè¯»å–PDFæ–‡æ¡£ï¼Œç›´æ¥ä¿å­˜åˆ°JSONæ–‡ä»¶"""
    try:
        all_text = ""
        pdf_folder = state["pdf_path"]
        classification = state.get("pdf_classification", {})

        # å…ˆå°è¯•ä»JSONæ–‡ä»¶è¯»å–ç°æœ‰å†…å®¹
        json_file_path = "pdf_content.json"
        existing_data = {}
        if os.path.exists(json_file_path):
            try:
                with open(json_file_path, "r", encoding="utf-8") as f:
                    existing_data = json.load(f)
                    all_text = existing_data.get("pdf_content", "")
                print(f"ğŸ“– è¯»å–ç°æœ‰JSONæ–‡ä»¶ï¼Œå½“å‰å†…å®¹é•¿åº¦: {len(all_text)} å­—ç¬¦")
            except Exception as e:
                print(f"âš ï¸ è¯»å–ç°æœ‰JSONæ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")

        for pdf_name in os.listdir(pdf_folder):
            # åªå¤„ç†éœ€è¦è§†è§‰è¯†åˆ«çš„PDF
            if classification.get(pdf_name) != "vision_needed":
                continue

            pdf_path = os.path.join(pdf_folder, pdf_name)
            print(f"ğŸ” ä½¿ç”¨QwenVLå¤„ç†: {pdf_name}")

            with fitz.open(pdf_path) as doc:
                for page_num in range(len(doc)):
                    page = doc[page_num]

                    # è½¬æ¢ä¸ºå›¾åƒ
                    image = convert_page_to_image(page)
                    result = extract_with_qwenvl(image)
                    if result["text"]:
                        # æ·»åŠ æ¥æºæ ‡è®°
                        all_text += f"[æ¥æº:{pdf_name}|é¡µç :{page_num + 1}]\n{result['text']}\n\n"

                    # å¦‚æœæœ‰è¡¨æ ¼æ•°æ®ï¼Œä¹Ÿæ·»åŠ è¿›å»
                    if result["tables"]:
                        for table in result["tables"]:
                            table_text = "\n".join(
                                [
                                    "\t".join(row)
                                    for row in [table.get("headers", [])]
                                    + table.get("rows", [])
                                ]
                            )
                            all_text += f"[æ¥æº:{pdf_name}|é¡µç :{page_num + 1}|è¡¨æ ¼]\n{table_text}\n\n"

            # æ·»åŠ æ–‡æ¡£ç»“æŸæ ‡å¿—
            all_text += f"[END:{pdf_name}]\n"

        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        from datetime import datetime

        pdf_data = {
            "pdf_content": all_text,
            "pdf_classification": classification,
            "timestamp": datetime.now().isoformat(),
            "total_length": len(all_text),
        }

        try:
            with open(json_file_path, "w", encoding="utf-8") as f:
                json.dump(pdf_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ PDFå†…å®¹å·²ä¿å­˜åˆ° {json_file_path}ï¼Œæ€»é•¿åº¦: {len(all_text)} å­—ç¬¦")
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")

        # åŒæ—¶è®¾ç½®åˆ°stateä¸­
        state["pdf_content"] = all_text
        print(
            f"ğŸ” QwenVLå¤„ç†å®Œæˆï¼Œæ–°å¢å†…å®¹é•¿åº¦: {len(all_text) - len(existing_data.get('pdf_content', ''))} å­—ç¬¦"
        )

    except Exception as e:
        print(f"QwenVLå¤„ç†PDFæ–‡æ¡£æ—¶å‡ºé”™: {e}")

    return state


def pdf_read(state: RagGraph):
    """è¯»å–PDFæ–‡æ¡£ï¼Œç›´æ¥ä¿å­˜åˆ°JSONæ–‡ä»¶"""
    try:
        all_text = ""
        pdf_folder = state["pdf_path"]
        classification = state.get("pdf_classification", {})

        # å…ˆå°è¯•ä»JSONæ–‡ä»¶è¯»å–ç°æœ‰å†…å®¹
        json_file_path = "pdf_content.json"
        existing_data = {}
        if os.path.exists(json_file_path):
            try:
                with open(json_file_path, "r", encoding="utf-8") as f:
                    existing_data = json.load(f)
                    all_text = existing_data.get("pdf_content", "")
                print(f"ğŸ“– è¯»å–ç°æœ‰JSONæ–‡ä»¶ï¼Œå½“å‰å†…å®¹é•¿åº¦: {len(all_text)} å­—ç¬¦")
            except Exception as e:
                print(f"âš ï¸ è¯»å–ç°æœ‰JSONæ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")

        for pdf_name in os.listdir(pdf_folder):
            if not pdf_name.endswith(".pdf"):
                continue

            # åªå¤„ç†å¯ç›´æ¥æå–æ–‡å­—çš„PDF
            if classification.get(pdf_name) != "text_extractable":
                continue

            pdf_path = os.path.join(pdf_folder, pdf_name)
            print(f"ğŸ“– ä½¿ç”¨pdfplumberå¤„ç†: {pdf_name}")

            with pdfplumber.open(pdf_path) as reader:
                for page in reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        # åœ¨æ¯ä¸ªæ–‡æ¡£å—å‰æ·»åŠ æ¥æºæ ‡è®°
                        all_text += f"[æ¥æº:{pdf_name}|é¡µç :{page.page_number}]\n{page_text}\n\n"
                all_text += f"[END:{pdf_name}]\n"

        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        from datetime import datetime

        pdf_data = {
            "pdf_content": all_text,
            "pdf_classification": classification,
            "timestamp": datetime.now().isoformat(),
            "total_length": len(all_text),
        }

        try:
            with open(json_file_path, "w", encoding="utf-8") as f:
                json.dump(pdf_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ PDFå†…å®¹å·²ä¿å­˜åˆ° {json_file_path}ï¼Œæ€»é•¿åº¦: {len(all_text)} å­—ç¬¦")
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")

        state["pdf_content"] = all_text
        print(
            f"ğŸ“– pdfplumberå¤„ç†å®Œæˆï¼Œæ–°å¢å†…å®¹é•¿åº¦: {len(all_text) - len(existing_data.get('pdf_content', ''))} å­—ç¬¦"
        )
    except Exception as e:
        print(f"è¯»å–PDFæ–‡æ¡£æ—¶å‡ºé”™: {e}")
        state["pdf_content"] = None
    return state


def extract_product_info(text: str, chunk_text: str = None) -> Dict[str, str]:
    """ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰å…ƒæ•°æ®ä¿¡æ¯ï¼šæ¥æºã€é¡µç ã€äº§å“ç¼–å·ã€æ‰¹æ¬¡å·"""
    import re

    # æå–æ¥æºä¿¡æ¯
    source_matches = re.findall(r"\[æ¥æº:([^|]+)\|é¡µç :(\d+)\]", text)
    pdf_name = source_matches[0][0] if source_matches else "æœªçŸ¥"

    # æå–é¡µç ä¿¡æ¯ï¼ˆä¼˜å…ˆä»chunk_textä¸­æå–ï¼‰
    page_num = "æœªçŸ¥"
    if chunk_text:
        page_matches = re.findall(r"\[æ¥æº:[^|]+\|é¡µç :(\d+)\]", chunk_text)
        if page_matches:
            page_num = page_matches[0]
    if page_num == "æœªçŸ¥" and source_matches:
        page_num = source_matches[0][1]

    # äº§å“ç¼–å·å’Œæ‰¹æ¬¡å·çš„æ¨¡å¼
    product_patterns = [
        r"äº§å“ç¼–å·[ï¼š:]*\s*([A-Z0-9\-]+)",
        r"äº§å“å·[ï¼š:]*\s*([A-Z0-9\-]+)",
        r"Product\s*No[.ï¼š:]*\s*([A-Z0-9\-]+)",
        r"Item\s*No[.ï¼š:]*\s*([A-Z0-9\-]+)",
        r"è´§å·[ï¼š:]*\s*([A-Z0-9\-]+)",
        r"ç¼–å·[ï¼š:]*\s*([A-Z0-9\-]+)",
    ]

    batch_patterns = [
        r"æ‰¹æ¬¡å·[ï¼š:]*\s*([A-Z0-9\-]+)",
        r"æ‰¹å·[ï¼š:]*\s*([A-Z0-9\-]+)",
        r"Batch\s*No[.ï¼š:]*\s*([A-Z0-9\-]+)",
        r"Lot\s*No[.ï¼š:]*\s*([A-Z0-9\-]+)",
        r"LOT[ï¼š:]*\s*([A-Z0-9\-]+)",
        r"ç”Ÿäº§æ‰¹æ¬¡[ï¼š:]*\s*([A-Z0-9\-]+)",
    ]

    # æå–äº§å“ç¼–å·ï¼ˆä¼˜å…ˆä»chunk_textä¸­æå–ï¼‰
    product_number = "æœªçŸ¥"
    search_text = chunk_text if chunk_text else text
    for pattern in product_patterns:
        match = re.search(pattern, search_text, re.IGNORECASE)
        if match:
            product_number = match.group(1).strip()
            break

    # å¦‚æœchunkä¸­æ²¡æ‰¾åˆ°ï¼Œå†ä»æ•´ä¸ªæ–‡æ¡£ä¸­æ‰¾
    if product_number == "æœªçŸ¥" and chunk_text:
        for pattern in product_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                product_number = match.group(1).strip()
                break

    # æå–æ‰¹æ¬¡å·ï¼ˆåŒæ ·çš„ç­–ç•¥ï¼‰
    batch_number = "æœªçŸ¥"
    for pattern in batch_patterns:
        match = re.search(pattern, search_text, re.IGNORECASE)
        if match:
            batch_number = match.group(1).strip()
            break

    if batch_number == "æœªçŸ¥" and chunk_text:
        for pattern in batch_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                batch_number = match.group(1).strip()
                break

    return {
        "source": pdf_name,
        "page": page_num,
        "product_number": product_number,
        "batch_number": batch_number,
    }


def get_chunks(state: RagGraph):
    """ä»JSONæ–‡ä»¶è¯»å–å†…å®¹å¹¶åˆ†å—"""
    # ä¼˜å…ˆä»JSONæ–‡ä»¶è¯»å–
    json_file_path = "pdf_content.json"
    raw_text = ""

    if os.path.exists(json_file_path):
        try:
            with open(json_file_path, "r", encoding="utf-8") as f:
                pdf_data = json.load(f)
                raw_text = pdf_data.get("pdf_content", "")
            print(f"ğŸ“– ä»JSONæ–‡ä»¶è¯»å–å†…å®¹ï¼Œé•¿åº¦: {len(raw_text)} å­—ç¬¦")
        except Exception as e:
            print(f"âŒ è¯»å–JSONæ–‡ä»¶å¤±è´¥: {e}")
            raw_text = state.get("pdf_content", "")
    else:
        raw_text = state.get("pdf_content", "")
        print("ğŸ“ ä½¿ç”¨stateä¸­çš„PDFå†…å®¹")

    if not raw_text:
        state["chunks"] = []
        print("ğŸ“ æ²¡æœ‰æ–‡æ¡£å†…å®¹éœ€è¦åˆ†å—")
        return state

    # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=200,
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ";", ":", "ï¼Œ", " ", ""],
    )

    # æŒ‰æ–‡æ¡£åˆ†å‰²
    doc_sections = raw_text.split("[END:")
    documents = []

    for section in doc_sections:
        if not section.strip():
            continue

        # åˆ†å‰²æ–‡æœ¬
        chunks = text_splitter.split_text(section)

        for chunk in chunks:
            # æå–æ‰€æœ‰å…ƒæ•°æ®ä¿¡æ¯
            metadata = extract_product_info(section, chunk)
            metadata["chunk_id"] = len(documents)

            # åˆ›å»ºDocumentå¯¹è±¡
            doc = Document(page_content=chunk, metadata=metadata)
            documents.append(doc)

    state["chunks"] = documents
    print(f"ğŸ“ æ–‡æ¡£å·²åˆ†å—ï¼Œå…± {len(documents)} ä¸ªå—")

    # æ˜¾ç¤ºæå–çš„ä¿¡æ¯ç»Ÿè®¡
    product_stats = {}
    batch_stats = {}
    for doc in documents:
        prod_num = doc.metadata.get("product_number", "æœªçŸ¥")
        batch_num = doc.metadata.get("batch_number", "æœªçŸ¥")
        product_stats[prod_num] = product_stats.get(prod_num, 0) + 1
        batch_stats[batch_num] = batch_stats.get(batch_num, 0) + 1

    print(
        f"ğŸ“Š äº§å“ç¼–å·ç»Ÿè®¡: {dict(list(product_stats.items())[:5])}{'...' if len(product_stats) > 5 else ''}"
    )
    print(
        f"ğŸ“Š æ‰¹æ¬¡å·ç»Ÿè®¡: {dict(list(batch_stats.items())[:5])}{'...' if len(batch_stats) > 5 else ''}"
    )

    return state


def vector_store_func(state: RagGraph):
    """æ„å»ºå‘é‡å­˜å‚¨ï¼Œå¤„ç†Documentå¯¹è±¡"""
    documents = state["chunks"]

    # å¤„ç†è¿‡é•¿æ–‡æ¡£
    valid_docs = []
    for doc in documents:
        if len(doc.page_content) <= 2000:
            valid_docs.append(doc)
        else:
            valid_docs.append(
                Document(
                    page_content=doc.page_content[:1900] + "...[æ–‡æœ¬è¢«æˆªæ–­]",
                    metadata=doc.metadata,
                )
            )

    if not valid_docs:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£å—")
        return state

    # æ„å»ºå‘é‡å­˜å‚¨
    vector_store = FAISS.from_documents(documents=valid_docs, embedding=embedding)
    vector_store.save_local("vector_store")
    state["vector_store"] = vector_store
    print(f"âœ… å‘é‡ç´¢å¼•å·²æ„å»ºï¼Œå¤„ç†äº† {len(valid_docs)} ä¸ªæ–‡æ¡£å—")

    return state


def retrieve(state: RagGraph):
    """ä»å‘é‡å­˜å‚¨ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œä½¿ç”¨äº¤å‰ç¼–ç å™¨é‡æ’"""
    print("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...")

    question = state.get("input", "")
    vector_store = state.get("vector_store")

    if not vector_store:
        vector_store = FAISS.load_local(
            "vector_store", embedding, allow_dangerous_deserialization=True
        )
        state["vector_store"] = vector_store
        print("âœ… æˆåŠŸåŠ è½½å·²æœ‰å‘é‡å­˜å‚¨")

    try:
        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ç›¸ä¼¼æ€§æœç´¢è·å–æ›´å¤šå€™é€‰æ–‡æ¡£
        candidate_docs = vector_store.as_retriever(
            search_type="similarity", search_kwargs={"k": 5}  # è·å–æ›´å¤šå€™é€‰æ–‡æ¡£ç”¨äºé‡æ’
        ).invoke(question)

        print(f"ğŸ“‹ è·å–åˆ° {len(candidate_docs)} ä¸ªå€™é€‰æ–‡æ¡£")

        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨qwen gte-rerank-v2è¿›è¡Œé‡æ’
        if candidate_docs:
            reranked_docs = rerank_with_qwen(question, candidate_docs)
            # å–ç¬¬ä¸€ä¸ªé‡æ’åçš„æ–‡æ¡£
            state["retrieved_docs"] = reranked_docs[:3]
            print(f"âœ… é‡æ’å®Œæˆï¼Œè¿”å›å‰ {len(state['retrieved_docs'])} ä¸ªç›¸å…³æ–‡æ¡£")
        else:
            state["retrieved_docs"] = []
            print("âŒ æœªæ‰¾åˆ°å€™é€‰æ–‡æ¡£")

        # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£æ¥æº
        sources = set()
        for doc in state["retrieved_docs"]:
            if hasattr(doc, "metadata") and "source" in doc.metadata:
                sources.add(doc.metadata["source"])

        if sources:
            print(f"ğŸ“š æ–‡æ¡£æ¥æº: {', '.join(sources)}")

    except Exception as e:
        print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
        state["retrieved_docs"] = []

    return state


def rerank_with_qwen(
    query: str, documents: List[Document], top_k: int = 10
) -> List[Document]:
    """ä½¿ç”¨qwen gte-rerank-v2å¯¹æ–‡æ¡£è¿›è¡Œé‡æ’"""
    try:
        # è®¾ç½®APIå¯†é’¥
        dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")

        # å‡†å¤‡é‡æ’æ•°æ®
        texts = []
        for doc in documents:
            content = doc.page_content if hasattr(doc, "page_content") else str(doc)
            # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…è¶…è¿‡æ¨¡å‹é™åˆ¶
            texts.append(content[:1000])

        # è°ƒç”¨é‡æ’APIï¼ˆæŒ‰ç…§å®˜æ–¹ç¤ºä¾‹ï¼‰
        resp = dashscope.TextReRank.call(
            model="gte-rerank-v2",
            query=query,
            documents=texts,
            top_n=min(top_k, len(texts)),
            return_documents=True,
        )

        if resp.status_code == HTTPStatus.OK:
            # æ ¹æ®é‡æ’ç»“æœé‡æ–°æ’åºæ–‡æ¡£
            reranked_docs = []
            for item in resp.output.results:
                index = item.index
                if index < len(documents):
                    reranked_docs.append(documents[index])

            print(f"ğŸ”„ é‡æ’æˆåŠŸï¼Œè¿”å› {len(reranked_docs)} ä¸ªæ–‡æ¡£")
            return reranked_docs
        else:
            print(f"âŒ é‡æ’APIè°ƒç”¨å¤±è´¥: {resp.status_code}, {resp.message}")
            return documents[:top_k]  # é™çº§åˆ°åŸå§‹æ’åº

    except Exception as e:
        print(f"âŒ é‡æ’è¿‡ç¨‹å‡ºé”™: {e}")
        return documents[:top_k]  # é™çº§åˆ°åŸå§‹æ’åº


def ai_answer(state: RagGraph) -> RagGraph:
    """æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ç”ŸæˆAIå›ç­”ï¼Œæ˜¾ç¤ºæ¥æºä¿¡æ¯"""
    print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...")

    question = state.get("input", "")
    docs = state.get("retrieved_docs", [])

    # æ„å»ºä¸Šä¸‹æ–‡å’Œæ¥æºä¿¡æ¯
    context_parts = []
    sources = set()

    for doc in docs:
        content = doc.page_content if hasattr(doc, "page_content") else str(doc)
        context_parts.append(content)

        # æ”¶é›†æ¥æºä¿¡æ¯
        if hasattr(doc, "metadata"):
            source = doc.metadata.get("source", "æœªçŸ¥")
            page = doc.metadata.get("page", "æœªçŸ¥")
            sources.add(f"{source}(é¡µç :{page})")

    context = "\n\n".join(context_parts)

    try:
        client = OpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        prompt = f"""æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š{context[:4000]}é—®é¢˜ï¼š{question}è¦æ±‚ï¼š1. åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”2. ç”¨ä¸­æ–‡å›ç­”3. ç®€æ´æ˜äº†"""

        response = client.chat.completions.create(
            model="deepseek-v3",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
        )

        # æ­£ç¡®æå–å›ç­”å†…å®¹
        if (
            hasattr(response, "choices")
            and response.choices
            and len(response.choices) > 0
        ):
            answer = response.choices[0].message.content
        else:
            answer = "ç”Ÿæˆå›ç­”å¤±è´¥"

        # æ·»åŠ æ¥æºä¿¡æ¯
        sources_text = "\nğŸ“š ä¿¡æ¯æ¥æºï¼š" + "ã€".join(sources) if sources else ""
        answer += f"\n\nğŸ“Š æ£€ç´¢ç»Ÿè®¡ï¼šå…±æ£€ç´¢åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ{sources_text}"

    except Exception as e:
        print(f"ç”Ÿæˆå¤±è´¥: {e}")
        sources_text = "ã€".join(sources) if sources else "æœªçŸ¥"
        answer = f"æ‰¾åˆ°ç›¸å…³å†…å®¹æ¥è‡ªï¼š{sources_text}\n\næ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µï¼Œä½†AIç”Ÿæˆå¤±è´¥ã€‚"

    state["output"] = answer
    return state


def load_test_questions(state: RagGraph):
    """åŠ è½½æµ‹è¯•é—®é¢˜é›†"""
    print("ğŸ“‹ åŠ è½½æµ‹è¯•é—®é¢˜é›†...")

    # ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨é¡¹ç›®ç›®å½•ä¸­çš„æ–‡ä»¶
    current_dir = os.path.dirname(os.path.abspath(__file__))
    test_file = os.path.join(current_dir, "coa_question.json")

    with open(test_file, "r", encoding="utf-8") as f:
        data = json.load(f)
        state["test_questions"] = data["test_questions"]
        state["current_question_index"] = 0
        state["evaluation_results"] = []

    print(f"âœ… å·²åŠ è½½ {len(state['test_questions'])} ä¸ªæµ‹è¯•é—®é¢˜")
    return state


def evaluate_single_question(state: RagGraph):
    """è¯„ä¼°å•ä¸ªé—®é¢˜"""
    questions = state["test_questions"]
    index = state["current_question_index"]

    if index >= len(questions):
        return state

    current_q = questions[index]
    question = current_q["question"]
    ground_truth = current_q["ground_truth"]
    relevant_docs = set(current_q["relevant_docs"])

    print(f"\nğŸ“ è¯„ä¼°é—®é¢˜ {index + 1}/{len(questions)}: {question[:50]}...")

    # è®¾ç½®å½“å‰é—®é¢˜åˆ°state
    state["input"] = question

    # æ‰§è¡Œæ£€ç´¢
    state = retrieve(state)

    # ç”Ÿæˆç­”æ¡ˆ
    state = ai_answer(state)

    # è®¡ç®—æŒ‡æ ‡
    retrieved_sources = set()
    for doc in state.get("retrieved_docs", []):
        if hasattr(doc, "metadata") and "source" in doc.metadata:
            retrieved_sources.add(doc.metadata["source"])

    # è®¡ç®—å¬å›ç‡å’Œå‡†ç¡®ç‡
    if relevant_docs:
        recall = len(retrieved_sources & relevant_docs) / len(relevant_docs)
    else:
        recall = 0.0

    if retrieved_sources:
        precision = len(retrieved_sources & relevant_docs) / len(retrieved_sources)
    else:
        precision = 0.0

    # è®¡ç®—F1åˆ†æ•°
    if precision + recall > 0:
        f1 = 2 * (precision * recall) / (precision + recall)
    else:
        f1 = 0.0

    # ä¿å­˜ç»“æœ
    result = {
        "question": question,
        "ground_truth": ground_truth,
        "generated_answer": state["output"],
        "relevant_docs": list(relevant_docs),
        "retrieved_docs": list(retrieved_sources),
        "precision": precision,
        "recall": recall,
        "f1": f1,
    }

    state["evaluation_results"].append(result)
    state["current_question_index"] += 1

    print(f"  ğŸ“Š P: {precision:.3f}, R: {recall:.3f}, F1: {f1:.3f}")

    return state


def calculate_final_metrics(state: RagGraph):
    """è®¡ç®—æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡"""
    print("\nğŸ“Š è®¡ç®—æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡...")

    results = state["evaluation_results"]

    if not results:
        state["metrics"] = {"precision": 0.0, "recall": 0.0, "f1": 0.0}
        return state

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_precision = sum(r["precision"] for r in results) / len(results)
    avg_recall = sum(r["recall"] for r in results) / len(results)
    avg_f1 = sum(r["f1"] for r in results) / len(results)

    state["metrics"] = {
        "precision": avg_precision,
        "recall": avg_recall,
        "f1": avg_f1,
        "total_questions": len(results),
    }

    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    report = f"""\nğŸ¯ è¯„ä¼°å®Œæˆï¼
ğŸ“ˆ æ€»ä½“æŒ‡æ ‡ï¼š
   å‡†ç¡®ç‡ (Precision): {avg_precision:.3f}
   å¬å›ç‡ (Recall): {avg_recall:.3f}
   F1åˆ†æ•°: {avg_f1:.3f}
   æµ‹è¯•é—®é¢˜æ•°: {len(results)}

ğŸ“‹ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° evaluation_results ä¸­"""

    state["output"] = report
    print(report)

    return state


def should_process_document(state: RagGraph) -> str:
    """æ¡ä»¶è¾¹ï¼šæ ¹æ®ç¼“å­˜çŠ¶æ€å’Œè¯„ä¼°æ¨¡å¼å†³å®šä¸‹ä¸€æ­¥"""
    if state.get("evaluation_mode", False):
        return "load_test_questions"
    return "retrieve" if state["cache_exists"] else "pdf_classify"


def should_continue_evaluation(state: RagGraph) -> str:
    """æ¡ä»¶è¾¹ï¼šåˆ¤æ–­æ˜¯å¦ç»§ç»­è¯„ä¼°"""
    questions = state.get("test_questions", [])
    index = state.get("current_question_index", 0)

    if index < len(questions):
        return "evaluate_single_question"
    else:
        return "calculate_final_metrics"


def should_route_pdf_processing(state: RagGraph) -> str:
    """æ¡ä»¶è¾¹ï¼šæ ¹æ®åˆ†ç±»ç»“æœè·¯ç”±åˆ°ä¸åŒçš„PDFå¤„ç†èŠ‚ç‚¹"""
    classification = state.get("pdf_classification", {})

    has_vision_needed = any(
        method == "vision_needed" for method in classification.values()
    )
    has_text_extractable = any(
        method == "text_extractable" for method in classification.values()
    )

    if has_text_extractable and has_vision_needed:
        return "pdf_read"
    elif has_text_extractable:
        return "pdf_read"
    elif has_vision_needed:
        return "pdfread_qwen"
    else:
        return "get_chunks"


def merge_pdf_content(state: RagGraph):
    """åˆå¹¶PDFå¤„ç†ç»“æœï¼ˆå†…å®¹å·²åœ¨å„è‡ªå‡½æ•°ä¸­ä¿å­˜åˆ°JSONï¼‰"""
    print("ğŸ“‹ PDFå¤„ç†ç»“æœå·²ä¿å­˜åˆ°JSONæ–‡ä»¶")
    return state


def create_smart_rag():
    """åˆ›å»ºæ™ºèƒ½RAGå·¥ä½œæµ"""
    workflow = StateGraph(RagGraph)

    # åŸæœ‰èŠ‚ç‚¹
    workflow.add_node("check_cache", check_cache)
    workflow.add_node("pdf_classify", pdf_classify)
    workflow.add_node("pdf_read", pdf_read)
    workflow.add_node("pdfread_qwen", pdfread_qwen)
    workflow.add_node("merge_content", merge_pdf_content)
    workflow.add_node("get_chunks", get_chunks)
    workflow.add_node("vector_store", vector_store_func)
    workflow.add_node("retrieve", retrieve)
    workflow.add_node("ai_answer", ai_answer)

    # è¯„ä¼°æ¨¡å¼èŠ‚ç‚¹
    workflow.add_node("load_test_questions", load_test_questions)
    workflow.add_node("evaluate_single_question", evaluate_single_question)
    workflow.add_node("calculate_final_metrics", calculate_final_metrics)

    # åŸæœ‰è¾¹å’Œæ¡ä»¶è¾¹
    workflow.add_conditional_edges(
        "check_cache",
        should_process_document,
        {
            "retrieve": "retrieve",
            "pdf_classify": "pdf_classify",
            "load_test_questions": "load_test_questions",
        },
    )

    workflow.add_conditional_edges(
        "pdf_classify",
        should_route_pdf_processing,
        {
            "pdf_read": "pdf_read",
            "pdfread_qwen": "pdfread_qwen",
            "get_chunks": "get_chunks",
        },
    )

    # è¯„ä¼°æ¨¡å¼çš„æ¡ä»¶è¾¹
    workflow.add_conditional_edges(
        "load_test_questions",
        should_continue_evaluation,
        {
            "evaluate_single_question": "evaluate_single_question",
            "calculate_final_metrics": "calculate_final_metrics",
        },
    )

    workflow.add_conditional_edges(
        "evaluate_single_question",
        should_continue_evaluation,
        {
            "evaluate_single_question": "evaluate_single_question",
            "calculate_final_metrics": "calculate_final_metrics",
        },
    )

    workflow.add_edge("pdf_read", "merge_content")
    workflow.add_edge("pdfread_qwen", "get_chunks")
    workflow.add_edge("merge_content", "pdfread_qwen")

    workflow.add_edge("get_chunks", "vector_store")
    workflow.add_edge("vector_store", "retrieve")
    workflow.add_edge("retrieve", "ai_answer")

    workflow.set_entry_point("check_cache")
    workflow.set_finish_point("ai_answer")
    workflow.set_finish_point("calculate_final_metrics")

    return workflow.compile()


if __name__ == "__main__":
    pdf_path = r"C:\Users\Yu\Desktop\coa\COA"

    smart_rag = create_smart_rag()
    print("ğŸ¤– æ™ºèƒ½RAGç³»ç»Ÿå¯åŠ¨")
    print("ğŸ’¡ è¾“å…¥ 'eval' è¿›å…¥è¯„ä¼°æ¨¡å¼ï¼Œ'quit' é€€å‡º")

    while True:
        question = input("\nâ“ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ (æˆ– 'eval' è¯„ä¼°): ").strip()
        if question.lower() in ["quit", "exit", "é€€å‡º", "q"]:
            break

        if not question:
            continue

        # åˆ¤æ–­æ˜¯å¦è¿›å…¥è¯„ä¼°æ¨¡å¼
        evaluation_mode = question.lower() == "eval"

        state: RagGraph = {
            "input": question if not evaluation_mode else "",
            "pdf_path": pdf_path,
            "pdf_content": None,
            "chunks": None,
            "vector_store": None,
            "retrieved_docs": None,
            "output": None,
            "cache_exists": False,
            "pdf_classification": {},
            "evaluation_mode": evaluation_mode,
            "test_questions": [],
            "current_question_index": 0,
            "evaluation_results": [],
            "metrics": {},
        }

        # åœ¨mainå‡½æ•°ä¸­å¢åŠ é€’å½’é™åˆ¶é…ç½®
        result = smart_rag.invoke(state, config={"recursion_limit": 500})

        if evaluation_mode:
            print(f"\n{result['output']}")
            # å¯é€‰ï¼šä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
            with open("evaluation_detailed_results.json", "w", encoding="utf-8") as f:
                json.dump(result["evaluation_results"], f, ensure_ascii=False, indent=2)
            print("ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° evaluation_detailed_results.json")
        else:
            print(f"\nğŸ¤– å›ç­”: {result['output']}")
